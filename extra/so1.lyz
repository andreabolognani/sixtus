title#Sistemi Operativi 1#Tutta la yadda di Crispone
prev#Extra/Scimmia/#La scimmia celeste
next#Extra/Immagini/#La lista delle immagini
start#page
tab#intro
	La yadda del Crispone per SO1.

	Il sistema operativo è quel programmone enorme e complesso che sta sotto a
	tutto, controlla quel che succede, alloca tutte le risorse e provvede modi
	comodi (?) per fare le cose, per comunicare e cazzeggiare. Ma in realtà, dal
	punto di vista della CPU, è soltanto un programma come gli altri.

	Solitamene appoggiato sopra un sottolissimo strato firmware/software specifico
	per l'hardware sottostante, il SO è un baraccone che può essere strutturato a
	livelli orizzontali (uno sull'altro) o verticali (le funzionalità sono
	indipendenti e vanno dall'utente all'hardware).

	Ora, la risorsa più importante per un processo che deve girare è la CPU. Ogni
	processo deve prenderla per fare le sue cose, ma la deve anche cedere agli
	altri. Per mantenere lo stato, il sistema mantiene un bel po' di dati per ogni
	processo in una grossa tabella; dentro ci stanno lo stack pointer, l'instruction
	pointer, i registri e una marea di (importantissime) cazzatine. Poi, in realtà,
	ci sono altre grosse tabelle per varie cose (la memoria principale, i
	dispositivi, i file aperti…).

	Torniamo al processo, ch'è importante.
tab#i
	title#Il processo
	Fondamentalmente, quasi tutto quel che gira è un processo, quindi i processi
	devono funzionare bene. Una delle cose più importanti è garantire la
	protezione della memoria, dunque: visto ch'è condivisa, è importante che
	nessuno tocchi la roba degli altri.

	La struttura dati del processo è un blocco di memoria che contiene il testo
	del processo, i suoi dati, il suo stack (anche più d'uno) e il suo heap e
	poi il <strong>PCB – Process Control Block</strong>, una struttura piena di
	metadati come la priorità, il PID, i registri, lo stato, i file aperti, i
	blocchi di memoria… possono anche esserci il PPID, l'utente, l'evento da
	attendere, una casella per i messaggi, un registro delle risorse allocate.

	Un'altra cosa estremamente importante è il modo d'esecuzione, uno tra utente
	e sistema, o privilegiato/non privilegiato (anche se possono essercene anche
	quattro, non li usa nessuno). La differenza è importante: in modalità utente
	sono in vigore molti divieti, e molte operazioni sono proibite; in modalità
	sistema, invece, si può far tutto (solo il kernel gira così).

	Alla creazione di un nuovo processo, il sistema si occupa di caricare in
	memoria il testo, linkare tutto quello ch'è dinamico, creare questa
	struttura appena citata ed effettuare tutte le registrazioni necessarie
	(padri, figli, liste di scheduling).

	stitle#Stati d'un processo
	Un processo può essere in esecuzione oppure no. Ma quando non lo è, può
	esserlo per molti ma molti motivi: può essere in fase di creazione, può
	essere in attesa perché il tempo a sua disposizione è finito (quindi è
	pronto), ma può anche essere in attesa di un evento particolare. Oppure, può
	essere finito.

	Le attese capitano quando il tempo di scheduling è terminato, quando si
	richiede qualcosa di grosso (I/O) oppure quando la memoria
	<code>fault</code>a. Ovviamente, ci sono anche gli errori e le chiamate di
	sistema, quando ad esempio si richiede una risorsa.

	stitle#Cambio di stato
	Ogni qualvolta un processo passa da <em>running</em> ad un qualche stato
	d'attesa, è necessario effettuare una serie di operazione per il cambio.
	Tutto quel che serve (registri, stack pointer, instruction pointer,
	contesto) dev'essere salvato nel PCB, poi lo stesso PCB dev'essere messo in
	una qualche coda. Dopodiché, la CPU può essere assegnata a qualcun'altro.

	Ma il sistema operativo, dove/come funziona? Può essere eseguito fuori/sotto
	i processi, ma alcune parti possono effettivamente girare come processi di
	sistema; ch'è più comodo, più modulare, più versatile. L'importante è che
	qualcuno si occupi di dare/togliere la CPU con regolarità: il
	<strong>dispatcher</strong>.

	Il Dispatcher mantiene una certa quantità di code di processi, una per
	stato: i processi pronti, quelli in attesa di risorse, quelli in attesa di
	messaggi, e una con tutti. In realtà, è peggio.

	stitle#Scheduling
	C'è una coda (una collezione di code) di processi pronti in attesa della
	CPU, poi una serie di code per ciascun dispositivo di I/O. La CPU viene
	assegnata ad un processo pronto, finché non succede qualcosa di
	significativo, momento in cui il processo viene rimesso in attesa. Per
	garantire che tutti i processi vengano eseguiti e che la CPU abbia sempre da
	fare, esistono diversi algoritmi per scegliere a chi dedicare l'esecuzione.

	Esiste uno scheduler a lungo termine, che sceglie i processi pronti, ed uno
	a breve termine, che alloca la CPU. Il primo viene chiamato ogni tanto,
	mentre il secondo viene invoca in continuazione (e deve quindi esssere
	estremamete rapido).

	Per facicilitare il lavoro dello scheduler, un processo può essere
	caratterizzato come <code>I/O bound</code> (dedito allo scambio di dati, con
	frequenti ma brevi <code>burst</code> di calcolo) oppure come <code>CPU
	bound</code> (dedito ai calcoli, con usi prolungati della CPU).
tab#ii
	title#Comunicazione fra processi
	È importante che i processi possano condividere informazioni quando
	necessario. Esistono fondamentalmente due modi per comunicare: scambiando
	messaggi oppure condividendo una certa sezione di memoria. Tipicamente,
	questo produce uno scenario <em>produttore/consumatore</em>, su un buffer
	che può essere limitato oppure no. Il buffer è una soluzione comoda, ma non
	sempre disponibile (i processi dovrebbero essere parenti per avere memoria
	condivisa).

	È possibile scambiare messaggi, inviandoli e ricevendoli. Prima di farlo,
	però, bisogna accordarsi: scegliere un canale di comunicizione.
	L'implementazione per il passaggio delle informazioni cambia: può essere
	fatto su buffer condiviso, tramite bus… Esistono moltissimi parametri da
	settare: il canale può essere mono/bi-direzionale, il numero di processi per
	estremità è limitato a uno, a molti, a tutti? I messaggi hanno dimensione
	fissa? Il numero di canali aperti per processo è limitato?

	In ogni caso, per la comunicazione occorre un qualche indirizzo, come il
	numero di una mailbox, o di una porta. La mailbox c'è o dev'essere creata?

	Comunque. Se due diversi processi sono in ascolto sulla stessa mailbox, che
	succede? Ricevono entrambi lo stesso messaggio? Lo riceve solo il primo, e
	l'altro resta ad aspettare? Il mittente può/deve essere informato del
	ricevente?

	E come ci si sincronizza? L'invio e la ricezione sono bloccanti? Il mittente
	deve attendere che qualcuno riceva, o può procedere? Il ricevente aspetta
	indefinitamente, o viene notificato quando la mailbox è vuota? Tutto questo,
	in teoria, dipende dalla capacità della mailbox: s'è zero, il mittente
	consegna direttamente al (cioè aspetta il) ricevente; s'è limitata, il
	mittente aspetta solo quando la casella è piena; s'è illimitata, il mittente
	lascia e se ne va.

	titler#Socket
	La forma di comunicazione più comoda (anche via rete) è la socket, associata
	ad una porta logica su un particolare indirizzo IP. Sulla/dalla socket
	possono arrivare/essere mandati dati grezzi in quantità arbitraria, anche
	in/da remoto.

	titler#Pipe
	Alternativamente, si usano le pipe, ossia dei tubi. Da una parte la roba
	entra, dall'altra esce; ma forse no. Normalmente, un'estremità può essere
	scritta, l'altra letta.

	E ancora normalmente, le pipe sono anonime e condivise tra processi parenti.
	Esiste la possibilità di creare una pipe con un nome, in modo che sia
	accessibile da tutti.
tab#iii
	title#Thread
	Ma i processi sono grossi e ciccioni, e farne tanti costa e pesa. In più,
	spesso non è affatto necessario che un processo generi un discendente per
	effettuare operazioni semplici. Ecco quindi il <em>thread</em>, un oggetto
	parallelo all'interno di un processo. Pur avendo un proprio stato, dei
	propri registri e un proprio stack, il thread condivide il testo, le risorse
	e il resto con il suo processo. Creare un thread è un'operazione
	estremamente più leggera che creare un processo.

	Poiché spesso uno stesso programma deve fare più cose contemporaneamente,
	sullo stesso spazio di memoria, è più semplice avere thread che convivono,
	piuttosto che avere processi che devono comunicare in modi complessi.

	L'implementazione effettiva dei thread può essere effettuata su più livelli:
	a livello utente (tramite librerie come PThread, JavaThread, Win32Thread) o
	a livello di sistema. La mappatura può essere varia: uno/uno, molti/uno,
	molti/molti.

	Ecco poi qualche problema: creare un processo figlio copia tutti i thread
	attivi? E quando un thread deve terminare?

	title#Segnali
	Un'altra forma di comunicazione verso i processi e i thread sono i segnali.
	Un <code>signal-handler</code> gestisce tutta la baracca: un segnale arriva,
	viene consegnato al processo/thread e questi lo gestisce.

	titler#Pool di Thread.
	Siccome i thread sono leggeri, non è sempre necessario o utile creare solo
	quelli necessari. È buona pratica tenerne da parte una certa quantità,
	lasciandoli fermi ma sempre disponibili; quando una richiesta arriva, il
	primo thread libero viene dedicato a seguire una certa operazione. È anche
	possibile, in questo modo, limitare il numero di thread che un processo può
	richiedere.
tab#iv
	title#CPU Scheduling
	Vogliamo due cose: che la CPU abbia sempre da fare (che faccia cose utili
	per la maggior parte del tempo) e che tutti i processi siano serviti
	regolarmente (per terminare in fretta).

	Per questo argomento possiamo vedere ogni processo come un'alternanza di CPU
	burst e attese per I/O. Statisticamente, più breve è il burst, più è
	frequente.

	Ora, lo scheduler torna al comando quando un processo termina o cede
	volontariamente la CPU; a quel punto, esso deve scegliere un nuovo processo.
	Questo meccanismo sarebbe di per se sufficiente ad implementare il
	multitasking. Ma un processo burlone potrebbe anche tenersi la CPU per
	sempre. È per evitare questi scherzoni che il sistema mantiene sempre il
	controllo della CPU, tramite un timer hardware: ad intervalli regolari, il
	sistema effettua una prelazione e si riprende quel ch'è suo. Può anche
	rischedulare lo stesso processo.

	È anche importante che il sistema non dorma perché non sono i processi a
	spostare i propri PCB da una lista all'altra, ma è il sistema a farlo. In
	più, se in esecuzione c'è un processo meno prioritario di uno appena
	arrivato, è meglio che il primo ceda il passo.

	Non è sempre necessario avere uno scheduler <code>preemptive</code>. Quando
	però lo è, bisogna stare attenti a lasciare tutto in uno stato consistente
	per evitare dei malanni.

	Cose da considerare quando si valuta lo scheduling:
	begin#ul
		utilizzo: percentuale di tempo utilizzato dalla CPU

		throughput: numero di processi terminati per unità di tempo

		turnaround: tempo d'esecuzione totale per un qualche processo

		attesa: permanenza d'un processo nella coda dei processi pronti

		tempo di risposta: distanza tra l'inoltro della richiesta al calcolo
		della risposta
	end#ul

	stitle#First come, first served
	Molto semplicemente, possiamo schedulare prima chi prima è arrivato. In
	questo modo, processi lunghi generano lunghe attese, e tutto dipende
	dall'ordine di arrivo. Non funziona un granché, ma è semplice.

	stitle#Shortest jog first
	Il processo più rapido viene schedulato per primo. Se preemptive, si calcola
	il tempo rimasto più breve. Questa è la soluzione ottima per i tempi di
	risposta. Purtroppo, noi non conosciamo queste durate, le possiamo soltanto
	stimare.

	Teniamo in considerazione la durata dei burst passati (pesandoli con un
	alpha compreso tra zero e uno).

	stitle#Priorità
	Un indice di priorità viene assegnato ad ogni processo, e chi ha priorità
	più alta passa davanti. Calcolare la priorità è un'altra impresa: come
	facciamo? Il sistema può farlo valutando l'uso di (quali) risorse, in base
	all'utente che lancia il processo.

	Ci sono però dei pericoli: i processi a (molto) bassa priorità potrebbe
	restare fermi e far passare tutti (<em>starvation</em>). Per ovviare a
	questo, solitamente, si lasciano invecchiare i processi (la priorità sale
	con l'anzianità di servizio).

	stitle#Round Robin
	Il tempo di CPU viene quantizzato <em>q</em>. Dati <em>n</em> processi
	pronti, ciascun di essi prende <em>q/n</em> per eseguire la sua roba. In
	questo modo, ciascun processo non può aspettare più di un intero quanto.

	Purtroppo, il quanto dev'essere regolato con molta attenzione: s'è troppo
	lungo, l'attesa sale; s'è troppo breve, l'overhead per il cambio di contesto
	sovrasta l'uso utile di processore.

	stitle#Code multiple
	Ovviamente, è possibile mettere assieme tanti algoritmi: prendiamo due code,
	una per i processi in background e una per quelli in foreground. Poi ci
	destreggiamo su quale coda gestire.

	Possiamo servire la seconda coda soltanto quando la prima è vuota, oppure
	possiamo assegnare un tempo limitato a ciascuna e alternare.

	stitle#Code multiple con feedback
	Aggiungendo complessità, ecco un sistema a molte code, con algoritmi e
	parametri diversi. Un processo può inoltre passare da una coda all'altra.

	Bisogna sceglie come muovere i processi e dove metterli all'inizio.
	Normalmente, si parte con la massima priorità: il processo arrivato potrebbe
	essere molto breve; ma se esaurisce il suo primo (piccolo) quanto, lo si
	declassa alla seconda coda, dove riceverà (con calma) un secondo quanto, più
	lungo. Se ancora poi non basta, gli daremo uno slot ancora più lungo,
	dall'ultima coda.

	Due ultime cose: i sistemi moderni hanno più d'una CPU. Per semplicità, si
	mantiene uno scheduler diverso per ogni CPU, ma con le code in comune. Ma
	non è detto.
tab#v
	title#La sincronizzazione
	Avere tanti processi che si muovono assieme è una brutta cosa, perché
	possono succedere un sacco di casini. Ogni accesso ad un dato condiviso può
	produrre stati inconsistenti. Quando un processo esegue operazioni su un
	dato condiviso, è in <em>sezione critica</em>. È vitale che questa
	esecuzione sia esclusiva.

	È bene che ogni processo avente una sezione critica possa entrarvi entro un
	tempo finito, ed ogni possibile sequenza reale deve garantire lo stesso
	risultato, indipendentemente dalla velocità relativa dei processi coinvolti.

	Ottenere l'esclusiva è semplice: basta disattivare gli interrupt. Oppure,
	basta <code>spinlock</code>are su una variable di controllo, ma l'attesa
	potrebbe essere infinita. Non solo, c'è l'attesa attiva: se tutti aspettano
	attivamente, il tempo viene sprecato in massa.

	È bene evitare questa attesa attiva: grazie a Dijkstra abbiamo i semafori.
	Ai semafori si può arrivare o passare (entrambe atomiche). Il semaforo
	contiene un contatore che scende quando qualcuno aspetta e sale quando
	qualcuno passa. Se il semaforo è binario, lo chiamamo <em>mutex</em>.

	Il sistema può mettere a disposizione una furba implementazione di questi
	semafori. Poiché, fondamentalmente, un semaforo è una risorsa, esso può
	essere trattato come una coda di scheduling. A ciascun semaforo è associata
	una coda di processi in attesa: non appena un processo si mette in attesa al
	semaforo, questi viene infilato nella coda e qualcun altro viene schedulato
	(attesa passiva). Quando poi qualcuno passa (il semaforo viene rilasciato)
	il prossimo processo in coda verrà spostato nella coda dei processi pronti.

	Le due operazioni sono estremamente brevi, pertanto è possibile disabilitare
	gli interrupt durante la loro esecuzione.

	Purtroppo, questo meccanismo è semplice, quindi basta dimenticarsi un pezzo
	per ottenere un deadlock. Succede infatti che spesso un solo semaforo non
	basti, ma se ne debba usare più d'uno: metti caso che vengano richiesti e
	lasciati in ordine diverso…

	stitle#Lettori/Scrittori
	È simile al problema del produttore/consumatore, ma peggio. Ci servono due
	attese diverse: se non c'è posto, nessuno può scrivere; se non c'è niente,
	nessuno può leggere. In più, uno scrittore deve scrivere da solo, ma i
	lettori possono anche andare in gruppo. Quindi, ci serve sapere se il buffer
	è vuoto, s'è pieno oppure no. Il buffer è circolare, quindi ha due indici
	che indicano il primo e l'ultimo elemento utile; e siccome questi dati sono
	condivisi, serve una terza condizione.
	
	Lo scrittore deve aspettare finché c'è spazio, poi deve prendere il buffer e
	scriverci. Lo scrittore deve aspettare finché c'è qualcosa, poi prendere il
	buffer e leggere (ma non togliere). Ma anche meglio: i lettore e gli
	scrittori si alternano.

	stitle#I cinque filosofi
	Ci sono cinque filosofi sul tavolo, pensano molto e mangiano poco. Ma per
	mangiare, hanno bisogno di due bacchette. Prendiamo allora un
	<em>monitori</em>, ossia un insieme di operazioni ad accesso esclusivo: solo
	un filosofo alla volta può monitorare, verifica che i suoi vicini non siano
	affamati, prende le due bacchette, mangia e poi le posa.
tab#vi
	title#Deadlock
	Succede, ovviamente, che non tutti i processi sono amici. Quindi il sistema
	può trovarsi in situazioni in cui molti richiedono risorse che non sono
	disponibili.

	Se le richieste non possono essere soddisfatte, i rilasci (che avvengono
	dopo l'uso) non arrivano mai, e la gente muore di fame e campa cavallo che
	l'erba cresce.
	
	Fortunatamente, non è poi così facile bloccare un sistema:
	begin#ul
		una risorsa dev'essere esclusiva

		un processo occupa risorse mentre ne attende altre

		il rilascio di una risorsa è volontario

		l'attesa dev'essere circolare: almeno due processi vogliono qualcosa che
		già è allocato a qualcuno
	end#ul
	Dato un grafo con i processi e le risorse, i processi richiedono risorse
	mentre le risorse sono assegnate ai processi. Se nel grafo c'è un ciclo,
	potrebbe esserci deadlock: bisognerebbe verificare che tutti i processi
	coinvolti rientrino nel cerchio. La cosa non è poi così semplice.

	Il pericolo di deadlock può essere semplicemente ignorato, poiché molto
	raro; altrimenti, bisogno decidere se impedire l'arrivo del sistema in uno
	stato pericoloso, oppure trovare un meccanismo di rollback.

	Si potrebbe rendere ogni risorsa esclusiva, oppure obbligare le richieste ad
	un particolare ordine, oppure forzare il rilascio. Ogni processo dovrebbe
	dichiarare in anticipo quali (e quante) risorse intende utilizzare, ma
	questo non garantisce un buon uso del sistema.

	Alternativamente, si può utilizzare qualche meccanismo per risolvere il
	deadlock: si sceglie un processo coinvolto e lo si termina. Non è affatto
	garantito che la termina di uno o più processi sia sufficiente a risolvere
	l'intero problema. Si potrebbe allora effettuare un rollback forzato,
	togliendo le risorse a tutti i coinvolti e sperare che la situazione si
	risolva da sola.
tab#vii
	title#La memoria principale
	La seconda risorsa, dopo la CPU, è la memoria. Anch'essa dev'essere
	sfruttata per bene, perché tutti i processi devono esservi caricati per
	funzionare. Ma la RAM costa tanto, ed è quindi piuttosto limitata. Non tutti
	i processi attivi possono risiedere contemporaneamente in memoria (o forse?)

	Comunque, mettiamo che la memoria sia piena e un nuovo processo arrivi. Che
	facciamo? Prendiamo un processo in attesa e lo togliamo dalle balle,
	mettendolo dove c'è posto (su disco).

	Ma non è l'unico problema: le variabili, le destinazioni dei salti, le
	callback sono indirizzi di memoria. Come li gestiamo? L'immagine binaria di
	un eseguibile linkato deve contenere indirizzi consistenti. E se un processo
	esce dalla memoria per poi rientrarci, questi indirizzi andranno cambiati.

	Fortunatamente, i salti sono definiti a partire dalla base del testo del
	programma, quindi basta conoscere quell'offset e aggiungerlo. Ma ne usciamo
	domani, possiamo anche usare del binding dinamico, se abbiamo l'hardware
	adatto: esiste infatti la <strong>MMU – Memory Management Unit</strong> che
	si ricorda l'offset e lo aggiunge ad ogni richiesta.

	Quindi, ogni variabile o istruzione è identificata da un indirizzo (logico)
	compreso tra zero e la dimensione dell'immagine del processo. Basta
	conoscere la posizione in memoria della prima cella, valore che viene
	salvato in un registro dedicato, il <strong>MAR – Memory Access
	Register</strong>. Analogamente, l'esito di queste somme genera indirizzi
	(fisici). Esistono quindi due spazi, uno logico e uno fisico, che possono e
	spesso hanno dimensioni diverse.

	In più, l'uso delle librerie dinamiche permette di utilizzare meglio la
	memoria, tutti i processi risparmiano spazio e solo le librerie necessarie
	vengono caricate in memoria (e solo quando serve).
	stitle#Allocazione
	Se uno ha poca briga, il sistema si riserva una parte (bassa) della memoria
	per se e lascia il resto all'utente. Se uno ha un po' di briga, separa la
	memoria in partizioni fisse che vengono assegnate ai processi. Ma se le
	partizioni sono fisse, c'è dello spreco.

	Allora usiamo le partizioni variabili: ogni processo prende solo spazio
	necessario. Ma quando un processo se ne va, resta un buco, che difficilmente
	verrà riempito da un processo uguale. I nuovi processi vanno infilati dove
	c'è posto, riempiendo posti piccoli o grandi?

	In ogni caso, con l'andare del tempo, resteranno dei buchetti piccoli
	piccoli (frammentazione esterna). Come compattiamo la memoria? E quando?
	Possiamo spostare tutti verso un estremo, oppure spostare dove ci sono più
	buchi, oppure solo quando c'è bisogno di posto per qualcuno.
	stitle#Paginazione
	Oppure, vediamo il problema dall'altro lato: teniamo partizioni piccole, e
	mettiamo un pezzo di processo qui, uno lì… Le pagine (logiche) e i frame
	(fisici).

	In questo modo, non serve avere uno spazio contiguo, basta avere abbastanza
	frame liberi. Il sistema deve quindi tenere una complessa tabella di
	allocazione, che associa le pagine ai frame (e i processi alle pagine).

	Con questo sistema, gli indirizzi hanno bisogno di numero di pagina e
	offset. Questi vengono tradotto (dall'MMU) in indirizzi fisici, con numero
	di frame e offset e quindi in indirizzo assoluto.

	Siccome l'hardware (adeguatamente supportato) fa tutto questo da solo, il
	programma in esecuzione continua a vedere il suo intero spazio
	d'indirizzamento come contiguo, inoltre l'accesso ai frame può essere
	monitorato in modo da semplicifare la protezione della memoria.

	Non solo: la frammentazione esterna sparisce, quella interna viene limitata
	alla dimensione del frame. La dimensione del frame è particolarmente
	importante: essa è una potenza di due (per semplicificare il calcolo degli
	indirizzi assoluti), ma deve essere bilanciata per avere spazio per il
	programma e contemporaneamente mantenere limitato il numero di frame. Esiste
	infatti una tabella dei frame che indica quali frame sono associati a quale
	processo, e in che ordine, anche perché le pagine non presenti devono essere
	caricate.

	Una tabella delle pagine fa parte del contesto di ogni processo;
	modernamente, ogni processo è composto di centinaia o migliaia di pagine,
	quindi la tabella sta in una pagina. Consultare la tabella, quindi, richiede
	un secondo accesso alla memoria, e il numero di accessi totale raddoppia.

	Fortunatamente, esiste supporto hardware apposito anche per questo. Il
	<strong>PTBR – Page Table Base Register</strong> serve per tener traccia
	della pagina con la tabella attiva. Alternativamente, si può
	<code>cache</code>are questa tabella attraverso un buffer dedicato, il
	<strong>TLB – Table Look-a-side Buffer</strong>, che mantiene un certo
	numero di associazioni chiave/valore. Il confronto parallelo, in caso di
	hit, è molto rapido; in caso di miss, bisogna effettuare il caricamento
	dalla tabella in RAM. Al cambio di contesto, poi, tutti gli indirizzi devono
	cambiare: la tabella può essere caricata con i primi blocchi, con dei
	blocchi a caso, oppure essere lasciata vuota e lasciare che si riempia col
	tempo, in seguito alle richieste.

	Una versione più sofisticata permette di mantenere anche un'associazione tra
	le entry del TLB con il PID del processo, in modo che soltanto una parte del
	buffer sia a disposizione. Alcuni riferimenti possono sopravvivere al cambio
	di contesto, mentre alcuni possono essere riservati al sistema.

	I processi inparentati (e le librerie dinamiche) hanno in comune molto
	codice: le pagine possono essere le stesse.
	stitle#Più livelli di paginazione
	Considerato quanta memoria abbiamo adesso, tenere tutte le entry per le
	pagine a 4KB di uno spazio d'indirizzamento a 32b, possiamo trovarci con
	svariati milioni di entry, per una tabella enorme. Ecco dunque una pagina di
	primo livello, che contiene gli indirizzi delle pagine di secondo livello.
	Gli indirizzi logici sono partizionati in tre. Ma anche aggiungendo livelli
	di paginazione, con i 64b c'è bisogno di un sacco di spazio.

	Un'altra soluzione è l'<strong>IPT – Inverted Page Table</strong>, che
	mantiene i dati per i frame (e non le pagine) e non può crescere in
	dimensione. Il contenuto associa ad ogni frame il PID e il numerale della
	pagina. Questa tabella è (molto) piccola, ma la ricerca è lentuccia; si
	possono utilizzare altri buffer per accelerarla.
	stitle#Segmentazione
	Indipendentemente dalla loro posizione in memoria, un programma è
	logicamente diviso in segmenti: il testo, le costanti, le variabili, i dati,
	lo stack, lo heap. Tutti questi segmenti sono indicizzati nella <strong>ST –
	Segment Table</strong> con indirizzo e dimensione.

	Questo sistema semplifica molto la protezione dei dati: alcuni segmenti sono
	in sola lettura, altri sono scrivibili. I segmenti possono essere condivisi
	tra processi, ma hanno ciascuno una dimensione diversa: è opportuno quindi
	allocarli dinamicamente riempiendo i buchi (poiché sono mediamente piuttosto
	piccoli).

	Combinando le due tecniche, i segmenti possono essere paginati: gli
	indirizzi logici puntano alla pagina che contiene il segmento.
tab#viii
	title#La memoria virtuale
	Siccome la memoria principale è limitata, mentre i processi sono
	potenzialmente troppi, è bene costruire un meccanismo che permetta ai
	processi di non risiedere sempre in memoria, pur restando attivi.
	Consideriamo allora alcuni fatti:
	begin#ul
		il testo di un processo può essere molto grande, ma non viene utilizzato
		molto spesso; la gestione degli errori, ad esempio, non viene chiamata
		molte volte

		le strutture dati sono spesso più grandi del necessario

		le librerie dinamiche vengono chiamate saltuariamente
	end#ul
	L'idea è quindi di tenere in memoria solo quello che serve adesso. Questo
	significa non solo che possiamo lasciare parte di un processo fuori, ma
	anche che possiamo eseguire un processo più grande della memoria presente; e
	complessivamente, più processi possono essere presenti contemporaneamente,
	aumentando il throughput. C'è anche la possibilità, però, che la memoria si
	riempia di schifezza.

	Questo metodo prevede di tenere in memoria soltanto una ristretta selezione
	delle pagine di un processo, e di caricarle solo su richiesta. Un processo
	che accede ad una pagina non presente sta effettivamente richiedendo una
	risorsa, e viene quindi messo in attesa finché questa non è presente. Si può
	scegliere, volendo, di far partire un processo prima che le sue pagine siano
	caricate in memoria, lasciando che siano i susseguenti page-fault a
	caricarle quando servono.

	Anche per questo servizio fa comodo il supporto hardware, in particolare la
	tabella delle pagine deve avere un bit di validità per indicare la presenza
	della pagina in memoria.

	Ora, dobbiamo osservare come ogni singolo accesso al disco sia
	spaventosamente lento. Dobbiamo quindi garantire che il numero di richieste
	soddisfatte al volo (hit) sia altissimo. Altrimenti, tutto il guadagno in
	tempo va perduto. Avere pagine grandi, ad esempio, aiuta. Tenere la roba
	nella partizione di swap (fatta apposta) pure.

	Altra cosa: difficilmente la memoria viene lasciata libera (quando un
	processo termina); è estremamente più probabile che si debba fare spazio,
	sacrificando altre pagine. Come scegliere le vittime? Ad ogni ingresso,
	fondamentalmente, viene scelto uno spazio di destinazione, che deve prima
	essere rimesso su disco (solo s'è stata scritta, ovviamente: potrebbe essere
	in sola lettura). Come scegliere i frame e quanti assegnarne a ciascun
	processo impatta fortemente sulle prestazioni. L'obiettivo è sempre
	minimizzare il numero di page-fault.

	Come facciamo? Prendiamo tutti gli indirizzi richiesti da un processo, in
	ordine di richiesta, e vediamo a quali pagine appartengono (segando le
	richieste identiche consecutive). Possiamo usare un semplice algoritmo FIFO:
	le pagine residenti da più tempo vengono rimpiazzate; se però le pagine
	vengono accedute in sequenza, il numero di fault aumenta anziché diminuire).
	Possiamo scegliere di scartare la pagina che verrà usata nel tempo più
	remoto (futuro). Ma noi difficilmente conosciamo il futuro; possiamo
	considerare il passato (ch'è noto) e scegliere di scartare la pagina che
	abbiamo usato (letto o scritto). Ma dobbiamo, per questo, datare o
	conteggiare gli accessi alle pagine, e controllare qual è il meno recente
	(ricerca poco efficente): servirebbe del supporto hardware estremamente
	specializzato, ed estremamente costoso.

	Possiamo avvicinarci, usando un semplice bit d'utilizzo: tutte le pagine
	partono con il bit a zero, passando a uno quando vengono cambiate. La pagina
	residente in fondo alla FIFO viene scartata soltanto se non è mai stato
	usata, altrimenti si procede (fino a trovare un bit a zero, oppure si torna
	in cima). Solitamente l'hardware fornisce sia il bit d'utilizzo che quello
	della modifica: le pagine usate di recente o modificate hanno priorità su
	quelle più vecchie e non modificate.

	Un'altra variante prevede un certo spazio di frame liberi, usati come
	cuscino tra i frame utilizzati e il disco: il sistema si occupa di scambiare
	la vittima selezionata con uno di questi frame liberi; di tanto in tanto, il
	sistema libera questi frame cuscinetto mettendoli su disco.
	stitle#Allocazione dei frame
	Come facciamo a mettere un limite al numero di frame per ogni processo?
	Possiamo assegnarne un numero identico a tutti, proporzionale al numero di
	processi attivi; oppure, possiamo proporzionarlo in base alla dimensione del
	processo, o in base alla priorità; e come facciamo all'inizio? Teniamo il
	limite, e lasciamo che il processo prenda dai frame degli altri finché può?
	Oppure, gli assegnamo una pool e poi lui s'arrangia?

	L'allocazione globale dei frame può rubare pagine ad altri processi, e
	provocare molti fault alla ripresa di un processo. L'allocazione locale
	invece dipende tantissimo dal numero di frame a disposizione: i processi
	possono darsi la zappa sui piedi oppure occupare spazio che potrebbe essere
	destinato ad altri. In ogni caso si rischia che tutti i processi debbano
	rubare pagine dagli altri e tutti perdano tempo aspettando (trashing).

	Il sistema dovrebbe adattare l'ammontare di frame dedicate a ciasun processo
	in base al numero di processi presenti, bloccando i processi in più quando
	sono troppi.
tab#ix
	title#File System
	Quello che l'utente percepisce del sistema è spesso un modo per accedere ad
	una struttura di directory che contengono file, ossia informazioni
	memorizzate in modo permanente.

	Un file è una sequenza di bit, che può essere anche molto lunga, al quale
	sono associati molti metadati. Fondamentale, per l'utente, è il nome; per il
	resto, vi sono posizione, tipo, dimensione, permessi…
	stitle#Operazioni sui file
	Un file può essere creato o cancellato, può essere aperto per essere letto o
	scritto, in ordine o in una posizione specifica. In più, al file si può
	cambiare nome, posizione, lo si può copiare. Ma non solo: i file sono
	organizzati in directory e possono essere estremamente grandi, contare
	milioni di file ed occupare centinaia di GB. Ma noi vorremmo che l'accesso a
	file fosse abbastanza costante.

	Per farlo, teniamo le informazioni relative ad un file organizzate in
	directory. Ci mettiamo tutto: la posizione, la dimensione, il proprietario,
	i permessi, le date di creazione, dell'ultimo accesso e dell'ultima
	modifica; e ce le mettiamo in modo efficiente. È quindi estremamente
	importante che le directory funzioni, ecco perché – pur essendo esse file
	come gli altri – non è possibile accedervi direttamente, ma solo attraverso
	opportuni comandi del sistema, perché perdere i riferimenti ad un file
	significa cancellarlo.

	Il modo utilizzato per implementare la directory influisce sulle performace:
	in MS-DOS c'erano soltanto liste di entry a 32bit (ecco il perché del famoso
	limite 8+3), mentre su NTFS si usano alberi bilanciati.

	La tabella dei file può essere singola, e contenere tutto. Oppure, può
	esserci una master-table che ne contiene una per ciascun utente. Oppure,
	ciascuna directory può contenerne altre, in una struttura ad albero. Un file
	system parte dalla radice, ogni utente ha la sua home ed ogni processo viene
	eseguito in una directory corrente.

	Data questa struttura, esistono un pathname assoluto e uno relativo per ogni
	file (e directory). Tuttavia, la struttura ad albero impedisce la
	condivisione… fortunatamente, esistono i link. E se un link creasse un
	ciclo? Possibile. Il sistema si occupa di impedire cancellazioni cicliche.
	stitle#Accesso ai file
	È impensabile specificare il path per accedere ad un file per ogni
	lettura/scrittura, perché percorrere l'intero path può richiedere decine di
	accessi a disco. Per questo motivo, i file non possono essere letti o
	scritti direttamente, devono essere prima aperti: il sistema ricerca il file
	su disco, lo trasporta in memoria e poi provvede il resto delle operazioni,
	mantenendo sia il contenuto del file che i suoi metadati in memoria. La
	sincronizzazione (scrittura su disco) avviene soltanto alla chiusura del
	file (o comunque non ad operazione).
	stitle#Protezione
	Non tutti possono eseguire tutte le operazioni su un file. Per semplicità,
	le operazioni sono [lettura, scrittura, esecuzione] mentre gli utenti sono
	[proprietario, gruppo, altri].
	stitle#Implementazione
	I file, in qualche modo, bisogna scriverli. Ma come? Il disco, per quanto
	grande, è limitato. Il sistema deve mediare tra l'utente e il disco, ch'è un
	dispositivo condiviso che può essere acceduto soltanto a blocchi e in modo
	sequenziale. In più, sul disco risiede anche il codice del sistema
	installato, che deve essere identificabile al boot.

	Servono alcune strutture, allora.
	begin#ul
		il <strong>boot control block</strong> contiene le informazioni per
		l'avvio, e di solito sta all'inizio

		il <strong>volume control block</strong> contiene le informazioni sulla
		partizione, la dimensione, il numero di blocchi, la master file table

		le directory contengono le locazioni dei blocchi

		i <strong>file controlo block</strong> contengono i metadati e gli inode
	end#ul
	In più, il sistema deve mantere una tabella con tutti i file system aperti,
	i punti di mount… perché tanti diversi file system (locali ma anche remoti)
	possono essere montati sullo stesso file system virtuale, al quale si accede
	senza preoccuparsi di dove o come i dati siano salvati.

	Per quanto il disco sia diviso in blocchi (la più piccola unità
	manipolabile), dobbiamo decidere come usarli: possiamo memorizzare il file
	in blocchi contigui, dal primo disponibile in avanti; è semplice e l'accesso
	è rapido, ma soggetto a frammentazione esterna, cosa che può richiedere
	periodiche deframmentazioni. E se un file crescesse oltre lo spazio
	disponibile? Bisognerà spostarlo, oppure allocargli preventivamente dello
	spazio in eccesso; ma la frammentazione aumenta! Abbiamo quindi gli stessi
	problemi della memoria principale.

	Allora, possiamo usare una catena di blocchi, ciascuno dei quali conterrà
	l'indice del successivo. La frammentazione viene evitata, ma l'accesso è
	molto più lento; non solo: se un blocco venisse danneggiato, tutto il
	contenuto successivo all'errore andrebbe perso. Potremmo usare una coda
	doppiamente linkata, e scorrere l'intero disco in caso d'errore.

	Oppure, possiamo <code>cluster</code>e i blocchi, aumentando le dimensioni;
	a prezzo della frammentazione interna, ovviamente. E i blocchi liberi? Ai
	tempi, usavamo la <strong>FAT – File Allocation Table</strong>, un lungo
	vettore che conteneva informazioni per ogni blocco del disco, e che poteva
	essere copiata interamente in memoria; occupando moltissimo spazio,
	purtroppo. Inoltre, se il disco è particolarmente grande (e spesso lo è) la
	tabella cresce; ed ovviamente, la tabella va sincronizzata ad ogni
	scrittura.

	Oppure, possiamo indicizzare il disco: teniamo un po' di blocchi dedicati a
	mantenere gli indici di altri file. L'accesso è rapido, l'indice però può
	essere sprecato, perché moltissimi file sono molto piccoli; alcuni invece
	sono estremamente grandi. Possiamo concatenare i blocchi di indici, oppure
	costruire gli indici a due livelli. Sotto Linux, gli inode sono esattamente
	questo: blocchi che contengono sia gli attributi che gli indici, così
	composto: 10 blocchi vengono puntati direttamente, tre altri blocchi sono
	invece puntati (il primo contiene indici, il secondo puntantori a blocco, il
	terzo puntantori a puntatori a blocco). Sotto NFTS, il disco viene diviso in
	elementi da 1~4KB, tutti linkati all'inizio del disco, e referenziati dalle
	directory. Per i file piccoli, il contenuto sta dentro l'elemento,
	altrimenti si usano gli indici (a blocchi concatenati).
	stitle#Gestione dello spazio libero
	Tutti i blocchi liberi vanno indicizzati, per essere pronti all'uso.
	Possiamo usare un vettore di bit di validità per indicare lo stato di
	ciascun blocco, oppure concatenare i blocchi liberi in una lista. Possiamo
	ovviamente riservare dei blocchi per indicizzare quelli liberi, oppure
	utilizzare una lista che indichi anche il numero di blocchi consecutivi
	liberi dopo il primo.
	stitle#Mapping dei file in memoria
	Indipendentemente da queste scelte per l'allocazione, un grandissimo impatto
	sulle prestazione è dovuto al sistema e al modo in cui gestisce i file
	aperti. Se le letture/scritture avvengono in memoria anziché su disco, i
	tempi di risposta crollano.
tab#x
	title#Dischi
	Indipendentemente dal file system, sui dischi bisogna scrivere. Come e
	quando?

	C'è da considerare che, nel tempo, la velocità dei processori aumenta, così
	come la capacità dei dischi. La velocità di rotazione (perché i dischi
	girano) è più o meno costante. Ciascuno piano del disco è diviso in tracce e
	settori, che incrociati fanno i blocchi. I blocchi sono numerati in ordine,
	e per ciascuno è possibile calcolare una tupla di coordinate
	piatto/settore/traccia.

	Questo non è esattamente trasparente: le tracce esterne sono più lunghe
	delle tracce interne, inoltre vi possono essere difetti di fabbricazione ed
	ovviamente i blocchi possono danneggiarsi con il tempo. A mappare
	fisicamente il blocco sul disco è il controller del disco stesso.

	Per l'accesso ad un punto specifico del disco esistono due latenze diverse:
	il disco deve ruotare fino al settore indicato, e la testina deve scorrere
	fino al cilindro/traccia corretto. Entrambi questi movimenti sono lenti, è
	quindi opportuno schedulare le operazioni sul disco.

	Ma dato che sulla rotazione non possiamo fare niente (il disco gira sempre),
	possiamo agire sulla posizione della testina. Possiamo muovere la testina
	verso la destinazione in ordine di arrivo (dispendioso), verso la più vicina
	tra le disponibili (possibile starvation), considerando i cilindri su un
	buffer circolare (dall'esterno all'interno, saltando poi in cima).
	stitle#Formattazione
	Un disco non può essere usato se non viene prima formattato. Una parte del
	processo viene effettuata dal produttore, in modo che ciascun blocco sia
	identificabile; poi viene la formattazione logica, ad opera del sistema, che
	serve a riservare alcune aree (le tabelle, l'indice dei blocchi liberi,
	l'istallazione del sistema stesso.

	Molto importante è il blocco di boot, che deve essere localizzabile
	dall'hardware residente perché il sistema possa partire. Il disco potrebbe
	anche contenere una partizione di swap (che potrebbe anche essere mappata su
	file).
	stitle#Protezione
	Siccome i dati vengono salvati su disco in modo permanente, il disco è una
	risorsa estremamente importante: se il disco si rompesse, i dati andrebbero
	persi. Esistono sistemi di protezioni atti a garantire la salvezza dei dati,
	come il <strong>RAID – Redundant Array of Inexpensive Disks</strong>.

	Un sistema RAID è composto da più d'un disco, ma il sistema non lo sa; esso
	è di solito composto di almeno due dischi e un controller SCSI.

	Il RAID0 mantiene i dati in singola copia, ma <code>strip</code>pa i blocchi
	a strisce. Lo stesso blocco logico viene spalmato su settori identici su
	dischi diversi, in modo da parallelizzare la scrittura.

	Il RAID1 mantiene due copie dei dati: un disco originale e un disco
	specchio. In caso di guasto di un disco, l'altro subentra immediatamente.
	Ovviamente, la stessa soluzione può essere adottata con doppi array di
	dischi.

	Il RAID2 prevede la distribuzione dei singoli bit su vari dischi, e non
	viene usato poiché estremamente dispendioso. Spalmerebbe le singole word su
	vari dischi, aggiungendo anche un'ECC come la codifica di Hamming.

	Il RAID3 prevede un disco di parità accanto agli altri. Il RAID4 strippa i
	blocchi e mantiene un disco di blocchi di parità. Il RAID5 fa la stessa
	cosa, ma i blocchi di parità sono divisi sui dischi; è il più usato. Il
	RAID6 fa la stessa cosa, ma con un disco in più; in questo modo, può
	resistere anche alla rottura di due dischi assieme (astronomicamente
	improbabile).
tab#xi
	title#I/O e dispositivi
	Esistono numerosissime varietà di dispositivi di I/O, che comunicano via
	porta logica, via bus o via controllori. Le istruzioni possono essere
	dirette oppure mappate in memoria. C'è bisogno di sapere un sacco di cose,
	per le modalità d'accesso, per gli errori.

	I processi possono attendere attivamente i dispositivi, oppure possono
	essere rischedulati quando il dispositivo emette un interrupt; oppure, il
	dispositivo potrebbe agire direttamente in parallelo, su una particolare
	zona di memoria, senza l'intervento del processore.

	Le interruzioni vengono mantenute (dall'hardware) in una coda che la CPU
	controlla ad ogni istruzione, e serve tramite un opportuno vettore di
	chiamate; lo stesso meccanismo serve anche le eccezioni.

	Uno dei dispositivi hardware comodi è il DMA Controller, che permette di
	accedere alla memoria dal bus, senza passare per il processore. In questo
	modo, la CPU può restare libera (per fare altro) mentre un dispositivo
	richiede/invia dati.

	I dispositivi si differenziano per molti aspetti: possono essere ad accesso
	diretto o sequenziale, a caratteri o a blocchi, sincroni o asincroni,
	condivisi o dedicati, in scrittura o in lettura o entrambi. Esistono
	dispositivi di rete che rispondono in modo diverso, e il sistema deve
	mediare per fornire interfacce comodo.

	Un altro dispositivo importante è il timer, che fornisce misure precise sul
	tempo e può essere usato per far scattare eventi in momenti stabiliti.

	Tutti i dispositivi vanno regolati e schedulati, poiché possono essere
	condivisi. Il sistema mantiene una tabella dei dispositivi, a ciascuno dei
	quali viene associata una coda con le richieste dei processi. Non solo: le
	richieste possono essere schedulate singolarmente, oppure devono essere
	raggruppate per richiedente (la stampante, ad esempio).

	La maggior parte dei dispositivi può fallire e dare qualche errore: non
	tutti possono essere gestiti, ma molti si. Inoltre, i dispositivi vanno
	protetti: esistono programmi utente che potrebbe tentare di utilizzare
	malamente un dispositivo. Per questo motivo, tutte le istruzioni di I/O sono
	ad uso esclusivo del sistema.

	Gran parte del kernel è dedicata esclusivamente a mantenere in memoria lo
	stato dei dispositivi, i buffer di ciascuno, i driver…

	Per la lettura da dispositivo, il sistema deve occuparsi di raccogliere la
	richiesta del processo, determinare il dispositivo interessato, tradurre la
	richiesta, inviarla, attendere il responso, rendere i dati disponibili al
	richiedente e svegliare il processo interessato. E non sempre il kernel può
	controllare il dispositivo: a volte deve passare per un driver specifico.
	stitle#Stream
	Alcuni dispositivi (di rete) funzionano a stream, e possono essere letti e
	scritti full-duplex, spesso asincrono non bloccante.

	Molti dispositivi molto veloci richiedono molti servizi, e la CPU fatica. È
	per questo che i dispositivi tendono ad avere controllori a bordo, i quali
	possono provvedere internamente a parte del lavoro.
tab#xii
	title#Protezione
	Con tanti utenti sullo stesso sistema, molte cose possono andare male. Per
	questo, a ciascun utente viene concesso soltanto il minimo grado di
	privilegio per eseguire i suoi compiti, le attività vengono loggate e tutto
	il resto negato. Si dice <strong>dominio di protezione</strong> l'insieme di
	coppie risorse/privilegi.

	La stessa cosa è verificabile anche tramite matrice di accessi: si
	incrociano i domini con le risorse. Se l'operazione non è permessa, la
	richiesta viene bloccata e respinta. Gli utenti hanno ruoli che conferiscono
	loro dei privilegi, parte (o tutti) dei quali vengono ereditati dai processi
	lanciati.
tab#xiii
	title#Real Time
	Esistono ambiti in cui il tempo è importante: nel senso che esiste un tempo
	oltre il quale un risultato, seppur corretto, diventa inutile. Se il sistema
	deve garantire l'esecuzione di un processo entro una certa deadline, allora
	dev'essere costruito appositamente.

	I vincoli possono essere espliciti (eseguiti questo compito ogni tot ms)
	oppure impliciti (mantieni costante la velocità). Per rispettare i vincoli,
	il sistema non deve soltanto essere veloce, ma deve garantire che le
	richieste vengano servite in tempo.

	In base alle conseguenze del superamento della deadline, i sistemi sono
	<em>soft</em> (la qualità del servizio cala) o <em>hard</em> (i freni non
	partono e sbatto contro il muro).

	Lo scheduler del sistema dev'essere progettato con molta attenzione: dato
	un insieme di processi pronti, essi devono rispettare una deadline, quindi
	le priorità devono essere scelte bene. Possiamo usare molti sistemi, ma come
	valutarli? Dobbiamo provare, perché non siamo certi di quanto tempo consumi
	ciascuno.

	In più, dobbiamo considerare tutte le attese: i processi potrebbero
	bloccarsi su qualche risorsa. Succede che un task ad alta priorità (e
	deadline vicina) richieda una risorsa già in possesso di un task molto
	lento: a questo punto, la priorità sembra invertisi, perché quelli veloci
	devono aspettare quelli lenti. Di solito, la priorità del veloce viene
	trasferita al lento per risolvere la questione.

	Un sistema realtime è studiato appositamente, in particolare sullo scheduler
	(che sceglie il prossimo processo) e il dispatcher (che effettua il cambio
	di contesto). Siccome gli algoritmi sono buoni, ma la vita è dura, ogni
	esecuzione è ritardata da eventi difficilmente prevedibili (latenza): i
	tempi di scheduling, gli interrupt, lo stesso hardware. Per minimizzare i
	cambi e le incertezze, solitamente tutti i programmi girano in kernelspace,
	e tutti possono chiamare tutto.
stop#page
start#side
	tid#Presentazione#intro
	title#Sistemi Operativi 1
	begin#ol
		li#tid#Il processo#i
		li#tid#Comunicazione tra processi#ii
		li#tid#Thread#iii
		li#tid#CPU Scheduling#iv
		li#tid#Sincronizzazione#v
		li#tid#Deadlock#vi
		li#tid#Memoria#vii
		li#tid#Memoria Virtuale#viii
		li#tid#File System#ix
		li#tid#Dischi#x
		li#tid#I/O#xi
		li#tid#Protezione#xii
		li#tid#Real Time#xiii
	end#ol
stop#side
